{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b43675",
   "metadata": {},
   "source": [
    "# Imaging Detectors\n",
    "\n",
    "## The human eye\n",
    "\n",
    "\n",
    "<img src=\"img/Eye_Layout.jpg\" width=\"360\">\n",
    "\n",
    "The __lens__ of our eye is a remarkable piece of __adaptive optics__. The lens is made up of unusual elongated cells that have no blood supply, but obtain nutrients from the surrounding fluids, mainly the aqueous humour that bathes the front of the lens. Waste products are removed through these fluids as well. The shape of the lens can be altered by the relaxation and contraction of the ciliary muscles surrounding it, thus enabling the eye to focus clearly on objects at widely varying distances.  This is referred to as accomodation.  Thus, accommodation relies on elasticity of the elongated cells in the lens, which makes it easier to change focal distances. As we age, the crystalline lens loses its elasticity, which results in a condition called presbyopia.  \n",
    "\n",
    "<img src=\"img/EyeLensFibers.jpg\"  width=\"360\">\n",
    "\n",
    "\n",
    "The eye has two types of receptors: \n",
    "\n",
    "- cones (~ 7M): response to bright light and color\n",
    "\n",
    "- rods (~ 100M): only sensitive to dim light, not to color, on the edge of the retina\n",
    "\n",
    "\n",
    "<img src=\"img/rod+cones.gif\"  width=\"360\">\n",
    "\n",
    "\n",
    "The optical nerve creates a blind spot in our vision.  However, our brain has been trained to adapt and we do not preceive it.\n",
    "\n",
    "<img src=\"img/Rod_Cones_loc.png\"  width=\"360\">\n",
    "\n",
    "In other word, our retina is fairly complex ''sensor'', combining receptors with different light sensitivities and resolution.  If we were to represent it as a regular matrix of photosensitive detectors (light in a digital camera), it would look like the image below.  \n",
    "\n",
    "_Question_ Do you think the human eye suffers from aliasing?  Why?\n",
    "\n",
    "<img src=\"img/EyeResolution.png\"  width=\"360\">\n",
    "\n",
    "You can see that compared to a digital camera we are only using ''processing'' power where it matters.\n",
    "\n",
    "<img src=\"img/RetinalVsCCDImage.png\"  width=\"360\">\n",
    "\n",
    "Finally, our light sensitivity is not only controlled by the type of sensors and the opening of the pupil, but also by changing our chemistry.  To be optimal for dark environments, it takes about 30 minutes for our eye to accomodate.  This is one of the reasons why it is painful to have a flash light  shone in our eyes at night.\n",
    "\n",
    "<img src=\"img/EyeSensitivity.png\"  width=\"360\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63860975",
   "metadata": {},
   "source": [
    "## Photo-detectors\n",
    "\n",
    "In photodectors photons hit the sensor and generate charge (electrons).  The photo generated electrons (or photoelectons) \n",
    "\n",
    "\n",
    "### Characteristics\n",
    "\n",
    "#### Efficiency of photo conversion process\n",
    "\n",
    "<img src=\"img/Ando_Sona_QE.jpg\" width=\"360\">\n",
    "\n",
    "_Quantum efficiency:_ \n",
    "$\\eta_q$\n",
    "\\begin{align}\n",
    "\\eta_q = \\frac{N_{pe}}{N_p}\n",
    "\\end{align}\n",
    "\n",
    "$N_{pe}$: Number of emitted electrons, also referred as number of photoelectrons\n",
    "\n",
    "$N_p$: Number of absorbed photons\n",
    "\n",
    "_sensitivity_\n",
    "\n",
    "The sensitivity of a camera is the minimum light signal that can be detected. By convention this is the light level falling on camera pixel that produces a signal just equal to the camera noise. Hence the noise of a camera sets an ultimate limit on the camera sensitivity. Digital cameras are therefore often compared using their noise figures.\n",
    "\n",
    "#### Noise\n",
    "\n",
    "The contributors to the noise are the camera dark current, the readout noise, and the shot noise, quantified using their standard deviations ($\\sigma_{DC},\\, \\sigma_{RO},\\, \\& \\sigma_{SN}$), respectively).\n",
    "\n",
    "_Dark Current_ \n",
    "\n",
    "The camera dark current noise is the current produced by the photo-detector even in the absence of a light source.  It is the result of thermally generated electrons which build up on pixels without light.  It can be understood that the longer the exposure time, the more dark current can accumulate. Dark current noise the charge generated from dark current.\n",
    "It is expressed as:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma_{DC} = \\mu_{DC}\\times Q \n",
    "\\end{align}\n",
    "Here $\\mu_{DC}$ is the accumulation rate given in $\\text{e}^−/\\text{p}∕\\text{s}$ (with p: pixel) and $Q$ is the integration (or exposure) time in s. \n",
    "\n",
    "Dark current is equivalent of white (thermal) noise and is present across all types of sensors.  It can be reduced by cooling down the sensor via thermoelectric cooling (Peltier plate) or cryogenic cooling.\n",
    "\n",
    "For instance, a typical modern scientific camera has $\\mu_{DC} = 0.1\\, \\text{e}^−∕\\text{s}$.\n",
    "\n",
    "_Readout noise_ \n",
    "\n",
    "The readout noise $\\sigma_{RO}$ is the noise generated by the electronics of the camera when the charge accumulated in a pixel is converted into an electrical signal.  A simplistic view, to make an analogy with standard acquisition system, is that the readout noise is mainly generated during quantization of the signal by the analog/digital converter (ADC) and is provided by camera manufacturer. \n",
    "\n",
    "For instance, a typical modern scientific camera has a noise floor down to $\\sigma_{RO} = 1.0\\, \\text{e}^−$.\n",
    "\n",
    "_Shot Noise_ \n",
    "\n",
    "The shot noise is due to quantum nature of light and is computed as \n",
    "\\begin{align}\n",
    "\\sigma_{SN} = \\sqrt{N_{pe}}\n",
    "\\end{align}\n",
    "This quantity is typically very large in low signal data.\n",
    "\n",
    "\n",
    "#### Dynamic range\n",
    "_Dynamic Range_\n",
    "\n",
    "Dynamic Range is a measure of the maximum and minimum intensities that can be simultaneously detected in the same field of view. It is often calculated as the maximum signal that can be accumulated, divided by the minimum signal. This equates to the noise associated with reading the minimum signal. It is commonly expressed either as the number of bits required to digitise the associated signals or on the decibel scale.\n",
    "\n",
    "_Signal to Noise Ratio_\n",
    "\n",
    "A camera signal-to-noise ratio (SNR) is the amount of signal above the background noise level.  It can be defined in a number of ways including:\n",
    "\n",
    "Based on the pattern contrast, computed here experimentally as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{SNR} = \\frac{N_{pe,p}-N_{pe,v}}{\\sigma}\n",
    "\\end{align}\n",
    "where $N_{pe,p}$ and $N_{pe,v}$ are the peak and valley intensities and $\\sigma$ is the average standard deviation of the whole signal over a small area.  $\\sigma$ can be measured experimentally and related to $\\sigma_{SN}$, $\\sigma_{RO}$, and $\\sigma_{DC}$.\n",
    "\n",
    "In general for flow imaging, the SNR is defined theoretically as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{SNR}_I = \\frac{N_{pe}-B}{\\sqrt{\\sigma_{DC}^2+\\sigma_{RO}^2+\\sigma_{SN}^2}}\n",
    "\\end{align}\n",
    "where B is the background illumination, from stray scattered laser light (proportional to $I_\\lambda \\tau_p$) and ambient light (negligible for short enough exposures).  Here is the assumed that the light is coming from a laser pulse of intensity $I_\\lambda$ and pulse length $\\tau_p$.\n",
    "\n",
    "\n",
    "__Discussion: light and noise__\n",
    "\n",
    "The quantity of light is in a close relationship with image noise. If there is a lot of light available, it is important that the pixel processes the greatest possible number of photons, that is, a high number of electrons can be saved (high saturation capacity). If, however, little light is available, the pixel should be able to gather as many photons as possible, that is, have a large pixel surface, and convert the photons effectively into electrons (high quantum efficiency). The less light is available, the more important low dark noise becomes.\n",
    "\n",
    "To see this, let's simplify the SNR$_I$ formula, by neglecting the readout noise and background illumination $B$.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{SNR}_I = \\frac{N_{pe}}{\\sqrt{\\sigma_{DC}^2+\\sqrt{N_{pe}}^2}}\n",
    "\\end{align}\n",
    "\n",
    "_Case 1: lot of light_: Let's assume $N_{pe}$ is the max well capacity of the sensor as $N_{pe}=30,000$.  Let's do two cases: $\\sigma_{DC}= 5 \\& 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb9d6f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC =  5 173.13295704742836\n",
      "DC =  10 172.9171253112705\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def SNR_I(Npe,DC):\n",
    "    ''' Calculate the simplified SNR formula above\n",
    "    Arguments\n",
    "    ---------\n",
    "    Npe: number of photoelectrons \n",
    "    DC: Dark current noise (in electrons)\n",
    "    Returns\n",
    "    -------\n",
    "    y : SNR\n",
    "    '''    \n",
    "    y = 0\n",
    "    y = Npe/np.sqrt(DC**2+np.sqrt(Npe)**2)\n",
    "    y_out = y\n",
    "\n",
    "    return y_out\n",
    "\n",
    "N_pe = 30000\n",
    "DC = 5\n",
    "print('DC = ', DC, SNR_I(N_pe,DC))\n",
    "# if we double DC:\n",
    "DC = 10\n",
    "print('DC = ', DC, SNR_I(N_pe,DC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630fd39",
   "metadata": {},
   "source": [
    "This is change of less than 1%!\n",
    "\n",
    "_Case 2: light light_ Here the effect of the dark current will be more pronounced on the signal quality.  Let's use a $N_{pe} = 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b6f321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC =  5 8.94427190999916\n",
      "DC =  10 7.071067811865475\n"
     ]
    }
   ],
   "source": [
    "N_pe = 100\n",
    "DC = 5\n",
    "print('DC = ', DC, SNR_I(N_pe,DC))\n",
    "# if we double DC:\n",
    "DC = 10\n",
    "print('DC = ', DC, SNR_I(N_pe,DC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db5ccb6",
   "metadata": {},
   "source": [
    "This is a significant change in SNR!  The signal quality has worsened by ~30%!  It is therefore very important to optimize for dark current in low light applications.  Well-capacity is less important, because we expect to stay well below the saturation property of the sensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c0665",
   "metadata": {},
   "source": [
    "_Modulation Transfer Function, MTF_\n",
    "\n",
    "1951 USAF Resolution Test Targets used to characterize the response of imaging system.  The imager on the left has a higher resolution than the one of the right.\n",
    "<img src=\"img/CalibTarget.jpg\" width=\"360\">\n",
    "\n",
    "Digital cameras have finite minimum regions of detection (pixels), that set a limit on the spatial sesolution of a camera. However the spatial resolution is affected by other factors such as the quality of the lens or imaging system. The limiting spatial resolution is commonly determined from the minimum separation required for discrimination between two high contrast objects, e.g. white points or lines on a black background, such as the 1951 USAF resolution target above. Contrast is an important factor in resolution as high contrast objects (e.g. black and white lines) are more readily resolved than low contrast objects (e.g. adjacent gray lines). The contrast and resolution performance of a camera can be incorporated into a single specification called the Modulation Transfer Function (MTF).  This will be covered in more details later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbd07d",
   "metadata": {},
   "source": [
    "### Photo-Multiplier tube\n",
    "\n",
    "<img src=\"img/PMT.png\" width=\"360\">\n",
    "\n",
    "A PMT consist of:\n",
    "\n",
    "Photocathode: It absorbs photons and emits electrons\n",
    "\n",
    "Dynods: It increase the number of electrons emitted by the photocathode, in other words, it amplifies the weak signal.  The voltage across the dynods controls the amplification factor.\n",
    "\n",
    "Anode: This is very the output charge is read after amplification.\n",
    "\n",
    "PMTs are used for low light, single photon, and very fast detection. They are typically used in LIDAR to count time of arrivals of individual photons, sensitive applications, such as nuclear radiation detection, biofluidic application (techniques such as FRAP, etc).\n",
    "\n",
    "### Photodiode\n",
    "\n",
    "<img src=\"img/Photodiode.png\" width=\"360\">\n",
    "\n",
    "Photodiodes are semiconductors.  They are based on a p-n junctions of semiconductors, commonly silicon-silican type, but other type such as InGaAs exists that have different spectral response.   Photodiodes have high quantum efficiency, but do not enable internal amplification.  They are the fundamental components behind camera pixel, which are 2D matrices of photodiodes.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c8365",
   "metadata": {},
   "source": [
    "## Digital Cameras\n",
    "\n",
    "\n",
    "<img src=\"img/CameraPixel.png\" width=\"360\">\n",
    "There are typically physical space between pixel to enable isolation of the charges collected by each pixel.  The occupied photosensitive surface area is called the fill factor.  With the exception of custom chips made e.g. by NASA for telescope or space applications where it can be as high as 100%, the fill factor is typically on the order of 50%.  To increase the light collected by the active part of each pixel, microlenses are typically mounted above each pixel.  This effectively increases the sensor sensitivity by collecting more photons at the price of slight reduction in spatial distortion (the microlens can introduce aberrations). \n",
    "\n",
    "In most modern sensor, color imaging is obtained by using a RGB Bayer filter, which assigns a color to each pixel in a $2\\times 2$ superpixel configuration.\n",
    "\n",
    "There are two main types of pixels: CCD and CMOS.\n",
    "\n",
    "__CCD or charge-couple device__:  In a CCD each pixel charge is transferred through a limited number of output nodes (often just one) to be converted to voltage, buffered, and then sent off-chip as an analog signal where it will be converted to a digital signal by an ADC.  This enables to have all the pixel dedicated to signal capture and obtain very high output uniformity (i.e. each pixel have very similar sensitivity).  However, this limits the acquisition (or frame) rate of the imager.\n",
    "\n",
    "__CMOS or Complementary metal-oxide semiconductors__ have a very different architecture.  Each pixel has its own charge-to-analog-voltage conversion.  The sensor can also include amplifiers, noise-corrections, and ADCs.  In other words, the chip output is now digital bits.  As a result of this integration of electronic components within the chip dye, there is less area dedicated for the photosensitive area, in other words, CMOS sensors will have a smaller fill factor than CCDs.  Because each pixel does its own charge to voltage conversion, the signal uniformity is lower than CCDs.  CMOS based cameras are also cheaper as they require less off-chip components. They are higher speed, but lower quality than CCDs.  \n",
    "\n",
    "<img src=\"img/cmoschips.jpg\" width=\"360\">\n",
    "\n",
    "\n",
    "However, CMOS technology is evolving (still) very quickly with the introduction of back-illuminated sensors, that stack the electronic vertically in the dye and thus enable higher fill factor.  Scientific CMOS, or sCMOS, are\n",
    "\n",
    "<img src=\"img/SonyBSI_CMOS.png\" width=\"360\">\n",
    "\n",
    "<img src=\"img/SonyBSI_CMOS_sidewview.png\" width=\"360\">\n",
    "\n",
    "\n",
    "CCD, EMCCD, ICCD and InGaAs cameras all have one readout structure, into which charge from the entire pixel array is converted. This means that any read noise follows a Gaussian distribution, with a peak read noise for the detector. sCMOS detectors, however, have one readout structure for every pixel column. This results in the read noise following a skewed histogram rather than a Gaussian distribution. Hence, read noise for sCMOS detectors is quoted as both root mean square (RMS) and median on the datasheet. RMS is more representative of the actual read noise.\n",
    "\n",
    "\n",
    "__Rolling vs global shutter__\n",
    "\n",
    "Depending on the chip architecture all the pixels can be exposed at once (global shutter) or sequentially row by row (rolling shutter).  This second mode can lead to distortions for fast moving objects!\n",
    "\n",
    "<img src=\"img/RollingVSGlobalShutter.jpg\" width=\"360\">\n",
    "\n",
    "### Frame rate\n",
    "\n",
    "The Frame Rate of a digital camera is the fastest rate at which subsequent images can be recorded and saved. Digital cameras can readout subsections of the image or bin pixels together to achieve faster readout rates, therefore typically two frame rates are defined, one is a full frame readout rate and the other is the fastest possible readout rate.\n",
    "\n",
    "Cameras to some degree all exhibit blemishes which affect the reproduction of the light signal. This is due to several variables, such as Gain variations across the sensor and regional differences in noise. These are compensated for with sophisticated software correction in more recent models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e668396",
   "metadata": {},
   "source": [
    "### Signal calculation\n",
    "\n",
    "A/D Conversion ($ADC$) in cnts/e-\n",
    "\n",
    "Total signal, $I$: \n",
    "\\begin{align}\n",
    "I = \\eta_q \\times ADC \\times N_{p}\n",
    "\\end{align}\n",
    "\n",
    "e.g. if incoming signal is $N_{p}=20$ photons/pixel, $\\eta_q=50\\%$, and $ADC = 0.2$ cnts/e$^-$: \n",
    "\n",
    "recorded signal will be: $20 \\times 0.5 \\times 0.2 = 2$ cnts\n",
    "\n",
    "Alternatively we can relate the measured signal (in counts) to the number of photons:  \n",
    "\n",
    "If $I=40$ cnts, then $N_p = I/(\\eta_q \\times ADC) = 40/(0.5 \\times 0.2) = 400$ photons\n",
    "\n",
    "## Image quality of cameras\n",
    "\n",
    "\n",
    "### Assessing the image quality by calculation of SNR\n",
    "\n",
    "Here we can reuse the first definition of SNR and apply it to images we will acquire.  This will necessitate to acquire two sets of images: bright images (near, but below the saturation limit) and dark images (with the lens cap on, for example).  In this case, we will define a region of interest (ROI) where the intensity is nearly uniform to perform our test.  The signal for the SNR calculation is the mean over that ROI, i.e. we need to compute the mean of $N_{pe}$, noted $<N_{pe}>$.  The new SNR definition becomes:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{SNR} = \\frac{<N_{pe}>|_{\\text{bright}}-<N_{pe}>|_{\\text{dark}}}{\\sigma(N_{pe})|_{\\text{bright}}}\n",
    "\\end{align}\n",
    "\n",
    "_example_:  You are comparing two different cameras.  A histogram of the bright, uniform image with both cameras is given below.  Assume the mean of the dark images are the same for both cameras.\n",
    "\n",
    "Which camera has the best image quality?\n",
    "<img src=\"img/SNR_Cam.png\" width=\"360\">\n",
    "\n",
    "### Other factors influencing image quality\n",
    "\n",
    "#### Quantity of light\n",
    "This will depend on the light source and camera lens being used.  Fundamental of camera lenses will be covered in anothe module.\n",
    "\n",
    "#### Sensor size\n",
    "Sensor sizes have been uniformized to simplify the implementation of imaging devices and selection of associated lenses.  The larger the sensor, the higher the expected image accuracy.\n",
    "#### Number of pixels\n",
    "Measured in megapixels (MPixels).  Naively, we would favor increasing the number of pixels for high resolution imaging.  However, large sensors are mores susceptible to noise.  We will also see in the camera lens module that cameras lenses have limited resolution and should be chosen carefully.  \n",
    "#### Pixel size\n",
    "The size of the pixel (with its fill factor) will determine how much light it can collect. Simply put a 5 $\\mu$m pixel will have 4 times the surface area of a 2.5 $\\mu$m pixel and therefore collect four times more light.  However, as we have seen before improvement in CMOS architecture (light back illuminated sensor) has enable sensors to improve very rapidly.  In fact a modern (circa 2023) 5 $\\mu$m pixel now outperform a 10 $\\mu$m pixel from 10 years ago.\n",
    "\n",
    "High resolution favors small pixels (as small as 1.4 $\\mu$m pixel today).  However, such pixel will have very low light sensitivity.\n",
    "\n",
    "#### Dynamics\n",
    "The dynamic range is the ratio between the strongest signal and weakest signal that can be detected.  It will be directly related by how many gray level a scene intensity can be discretized into.\n",
    "\n",
    "Additionally, sensors should have a linear response, or images might be biased, which is some scientific applications could be problematic. \n",
    "\n",
    "\n",
    "### Sample spec sheet for cameras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b9bd7",
   "metadata": {},
   "source": [
    "### Low signal acquisition\n",
    "\n",
    "For low signals, it might be necessary to use an image intensifier to increase the number of photoelectrons, and therefore signal count.  An image intensifier is made of a photocathode, micro-channel plate (MCP), phosophorus, and relay optics to couple the light to a detector (typically a CCD or low noise CMOS camera).\n",
    "\n",
    "<img src=\"img/ImageIntensifier.png\" width=\"360\">\n",
    "\n",
    "#### Signal calculation for ICCD vs CCD\n",
    "\n",
    "For this calculation we are taking an input signal of $N_p=20$ photons/pixel, which is typically the readout noise of CCD cameras.\n",
    "\n",
    "\\begin{array}{l l l l l}\n",
    "& & \\text{ICCD} & \\text{CCD} & \\text{Comments}\\\\\n",
    "\\hline\n",
    "\\text{input light} & 20 \\,\\text{photons/p} & & & \\lambda = 550\\, \\text{nm}\\\\\n",
    "\\eta_q \\,\\text{(photocathode)} & 5 \\% & 1 \\text{e}^- &  - & \\text{Only intensifier}\\\\\n",
    "\\text{Max MCP gain} & 300 \\text{e}^- & 300 \\text{e}^- & - & \\text{Only intensifier}\\\\\n",
    "\\text{Phosphor conversion} & 200 \\, \\text{photons/e}^- & 6 \\times 10^4\\,\\text{photons} & - & \\text{P43 phosphor, only intensifier}\\\\\n",
    "\\text{Relay collection efficiency} & 10\\% & 6\\times 10^3 \\,\\text{photons} & - & \\text{Only intensifier}\\\\\n",
    "\\eta_q \\,\\text{(CCD)} & 50\\% & 3 \\times 10^3 \\,\\text{e}^- & 10 \\,\\text{e}^- & \\text{QE determines noise}\\\\\n",
    "\\text{ADC (CCD)} & 0.2\\,\\text{cnts/e}^- & 600 \\,\\text{cnts} & 2  \\,\\text{cnts} & \\\\\n",
    "\\end{array}\n",
    "\n",
    "Overally, we used a very complex system, but the ICCD gives a 300 $\\times$ higher signal compared to the CCD.  The ICCD noise is always higher than the CCD camera noise due to the lower quantum efficiency of the intensifier and due to the additional statistical noise induced by the electron multiplication inside the MCP.\n",
    "\n",
    "\n",
    "#### ICCD resolution\n",
    " \n",
    "It is evident that the combination of imaging \"modules\" as CCD, image intensifiers and fiber optical tapers or lenses, of which each component has a finite image resolution, must reduce the overall image resolution compared to a CCD alone. Proximity focused image intensifiers have limited resolution of max. 50lp/mm9 (= 10 $\\mu m$/line). At this value the intensifier has a contrast of 5%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c55df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
